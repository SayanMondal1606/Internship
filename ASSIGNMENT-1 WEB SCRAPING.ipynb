{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93caad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-1\n",
    "# WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d0da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "From today's featured list\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# specify the url of the website and make a request\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the html using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find all header tags and print the text content\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "for header in headers:\n",
    "    print(header.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a438fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2960589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found.\n",
      "Empty DataFrame\n",
      "Columns: [Name, Term of Office]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a Beautiful Soup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the information about former presidents\n",
    "table = soup.find(\"table\", {\"class\": \"views-table\"})\n",
    "\n",
    "# Create lists to store the extracted data\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "# Extract the name and term of office for each former president\n",
    "table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "\n",
    "if table is not None:\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        data = row.find_all(\"td\")\n",
    "        name = data[0].text.strip()\n",
    "        terms = data[2].find_all(\"span\", class_=\"term\")\n",
    "        ...\n",
    "else:\n",
    "    print(\"Table not found.\")\n",
    "# Create a data frame from the extracted data\n",
    "df = pd.DataFrame({\"Name\": names, \"Term of Office\": terms})\n",
    "\n",
    "# Print the data frame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "# c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275ee1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0        India\\nIND      49  5,839    119\n",
      "1    Australia\\nAUS      36  4,015    112\n",
      "2     Pakistan\\nPAK      32  3,525    110\n",
      "3  South Africa\\nSA      29  3,166    109\n",
      "4   New Zealand\\nNZ      38  4,007    105\n",
      "5      England\\nENG      34  3,377     99\n",
      "6     Sri Lanka\\nSL      43  3,943     92\n",
      "7   Bangladesh\\nBAN      40  3,574     89\n",
      "8  Afghanistan\\nAFG      26  2,170     83\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "teams_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  teams = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  teams_data.append([teams, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(teams_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6492725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0             Babar Azam  PAK    829\n",
      "1           Shubman Gill  IND    823\n",
      "2        Quinton de Kock   SA    769\n",
      "3       Heinrich Klaasen   SA    756\n",
      "4           David Warner  AUS    747\n",
      "5            Virat Kohli  IND    747\n",
      "6           Harry Tector  IRE    729\n",
      "7           Rohit Sharma  IND    725\n",
      "8  Rassie van der Dussen   SA    716\n",
      "9            Imam-ul-Haq  PAK    704\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsmans_data = []\n",
    "tables = soup.find(\"table\", class_=\"table\")\n",
    "rowss = tables.find_all(\"tr\")\n",
    "\n",
    "for row in rowss[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsmans = cells[1].text.strip()\n",
    "  teams = cells[2].text.strip()\n",
    "  ratings = cells[3].text.strip()\n",
    "  batsmans_data.append([batsmans, teams, ratings])\n",
    "\n",
    "df = pd.DataFrame(batsmans_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0b487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Bowler Team Rating\n",
      "0  Josh Hazlewood  AUS    670\n",
      "1  Mohammed Siraj  IND    668\n",
      "2  Keshav Maharaj   SA    656\n",
      "3     Rashid Khan  AFG    654\n",
      "4     Trent Boult   NZ    653\n",
      "5   Mohammad Nabi  AFG    641\n",
      "6      Adam Zampa  AUS    635\n",
      "7      Matt Henry   NZ    634\n",
      "8   Kuldeep Yadav  IND    632\n",
      "9  Shaheen Afridi  PAK    625\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowlers_data = []\n",
    "tables = soup.find(\"table\", class_=\"table\")\n",
    "rowzs = tables.find_all(\"tr\")\n",
    "\n",
    "for row in rowzs[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowlers = cells[1].text.strip()\n",
    "  teams = cells[2].text.strip()\n",
    "  ratings = cells[3].text.strip()\n",
    "  bowlers_data.append([bowlers, teams, ratings])\n",
    "\n",
    "df = pd.DataFrame(bowlers_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf632527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "#a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7a491ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      19  3,084    162\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      18  1,610     89\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      11    816     74\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      21  1,435     68\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "soup = scrape_data(url)\n",
    "table = soup.find('table', class_='table')\n",
    "rows = table.tbody.find_all('tr')\n",
    "data = []\n",
    "for row in rows:\n",
    "     cols = row.find_all('td')\n",
    "     team = cols[1].text.strip()\n",
    "     matches = cols[2].text.strip()\n",
    "     points = cols[3].text.strip()\n",
    "     rating = cols[4].text.strip()\n",
    "     data.append([team, matches, points, rating])\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0abe7128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsmans_data = []\n",
    "tables = soup.find(\"table\", class_=\"table\")\n",
    "rowss = tables.find_all(\"tr\")\n",
    "\n",
    "for row in rowss[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsmans = cells[1].text.strip()\n",
    "  teams = cells[2].text.strip()\n",
    "  ratings = cells[3].text.strip()\n",
    "  batsmans_data.append([batsmans, teams, ratings])\n",
    "\n",
    "df = pd.DataFrame(batsmans_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f77735b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0        Marizanne Kapp   SA    385\n",
      "1      Ashleigh Gardner  AUS    377\n",
      "2  Natalie Sciver-Brunt  ENG    360\n",
      "3       Hayley Matthews   WI    358\n",
      "4           Amelia Kerr   NZ    346\n",
      "5         Deepti Sharma  IND    312\n",
      "6          Ellyse Perry  AUS    282\n",
      "7         Jess Jonassen  AUS    227\n",
      "8         Sophie Devine   NZ    227\n",
      "9              Nida Dar  PAK    224\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsmans_data = []\n",
    "tables = soup.find(\"table\", class_=\"table\")\n",
    "rowss = tables.find_all(\"tr\")\n",
    "\n",
    "for row in rowss[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsmans = cells[1].text.strip()\n",
    "  teams = cells[2].text.strip()\n",
    "  ratings = cells[3].text.strip()\n",
    "  batsmans_data.append([batsmans, teams, ratings])\n",
    "\n",
    "df = pd.DataFrame(batsmans_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame.\n",
    "#i) Headline\n",
    "#ii) Time\n",
    "#iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37839c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e546ef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline Time  \\\n",
      "0   Stocks making the biggest moves premarket: McD...        \n",
      "1   Stock futures rebound to start a big week with...        \n",
      "2   Treasury yields rise as investors look to Fed ...        \n",
      "3   Moscow claims the West is attempting to 'split...        \n",
      "4   G7 calls for immediate repeal of bans on Japan...        \n",
      "5   Israel-Hamas war is affecting the financial ou...        \n",
      "6            A new hope for Ukraine aid in Washington        \n",
      "7   Russia's obsession with Ukraine has weakened i...        \n",
      "8   Volvo Cars CEO strikes cautious tone on solid-...        \n",
      "9   Peak fossil fuel demand is coming, IEA chief B...        \n",
      "10  Global demand for oil, coal and gas set to pea...        \n",
      "11  America’s first major offshore wind farm insta...        \n",
      "12  The world's largest offshore wind farm produce...        \n",
      "13  Vietnam is at the 'leading edge' of AI develop...        \n",
      "14  The Israel-Hamas war hasn't affected Thai Airw...        \n",
      "15  Southeast Asia looks to renewable power for en...        \n",
      "16  What Thailand's new coalition government means...        \n",
      "17  Southeast Asia is set to drive up demand for g...        \n",
      "18  Is your hotel sustainable? Not if these two th...        \n",
      "19  The best places — and best times — to take a s...        \n",
      "20  Revenge travel is over — even in China, says C...        \n",
      "21  He visited every country in the world — withou...        \n",
      "22  These are the world's 50 best bars in 2023 — a...        \n",
      "23  McDonald's HR exec: The No. 1 skill you get wo...        \n",
      "24  The top 10 best states for millennials—New Yor...        \n",
      "25  The No. 1 most ‘overlooked’ skill kids with hi...        \n",
      "26  This 51-year-old makes an average of $10,000 p...        \n",
      "27  Mark Cuban shares the 9-word mantra he learned...        \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/10/30/-stocks-making...  \n",
      "1   https://www.cnbc.com/2023/10/29/stock-market-t...  \n",
      "2   https://www.cnbc.com/2023/10/30/us-treasurys-i...  \n",
      "3   https://www.cnbc.com/2023/10/30/ukraine-war-li...  \n",
      "4   https://www.cnbc.com/2023/10/30/g7-calls-for-i...  \n",
      "5   https://www.cnbc.com/2023/10/28/israel-hamas-w...  \n",
      "6   https://www.cnbc.com/2023/10/27/a-new-hope-for...  \n",
      "7   https://www.cnbc.com/2023/10/26/russias-influe...  \n",
      "8   https://www.cnbc.com/2023/10/27/volvo-cars-ceo...  \n",
      "9   https://www.cnbc.com/2023/10/27/peak-fossil-fu...  \n",
      "10  https://www.cnbc.com/2023/10/24/demand-for-fos...  \n",
      "11  https://www.cnbc.com/2023/10/19/americas-first...  \n",
      "12  https://www.cnbc.com/2023/10/09/the-worlds-lar...  \n",
      "13  https://www.cnbc.com/video/2023/10/30/vietnam-...  \n",
      "14  https://www.cnbc.com/video/2023/10/23/the-isra...  \n",
      "15  https://www.cnbc.com/2023/10/17/southeast-asia...  \n",
      "16  https://www.cnbc.com/2023/10/10/what-thailands...  \n",
      "17  https://www.cnbc.com/2023/10/03/southeast-asia...  \n",
      "18  https://www.cnbc.com/2023/10/30/is-your-hotel-...  \n",
      "19  https://www.cnbc.com/2023/10/25/the-best-place...  \n",
      "20  https://www.cnbc.com/2023/10/24/revenge-travel...  \n",
      "21  https://www.cnbc.com/2023/10/19/44-year-old-tr...  \n",
      "22  https://www.cnbc.com/2023/10/18/what-are-the-5...  \n",
      "23  https://www.cnbc.com/2023/10/29/mcdonalds-hr-e...  \n",
      "24  https://www.cnbc.com/2023/10/29/best-states-mi...  \n",
      "25  https://www.cnbc.com/2023/10/29/kids-with-high...  \n",
      "26  https://www.cnbc.com/2023/10/29/nora-curl-make...  \n",
      "27  https://www.cnbc.com/2023/10/29/mark-cuban-lea...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "# Loop through each article and extract the headline, time, and link\n",
    "for article in articles:\n",
    "    headline = article.find('a').text.strip()\n",
    "    time_element = article.find('time')\n",
    "    if time_element is not None:\n",
    "        time = time_element['datetime']\n",
    "    else:\n",
    "        time = ''\n",
    "    link = article.find('a')['href']\n",
    "\n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    links.append(link)\n",
    "\n",
    "# Create a data frame from the extracted details\n",
    "df = pd.DataFrame({'Headline': headlines, 'Time': times, 'News Link': links})\n",
    "\n",
    "# Print the data frame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame.\n",
    "#i) Paper Title\n",
    "#ii) Authors\n",
    "#iii) Published Date\n",
    "#iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39e995e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch the webpage's HTML content\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all the articles on the page\n",
    "articles = soup.find_all(\"div\", class_=\"pod-listing-header\")\n",
    "\n",
    "# Loop through each article and extract the required information\n",
    "article_data = []\n",
    "for article in articles:\n",
    "    # Extract the paper title\n",
    "    title = article.find(\"a\").text.strip()\n",
    "\n",
    "    # Extract the authors' names\n",
    "    authors = \"\"\n",
    "    for author in article.find_all(\"a\", class_=\"author-name\"):\n",
    "        authors += author.text.strip() + \", \"\n",
    "    authors = authors.rstrip(\", \")\n",
    "\n",
    "    # Extract the published date\n",
    "    published = article.find(\"span\", class_=\"cover-date\").text.strip()\n",
    "\n",
    "    # Extract the paper URL\n",
    "    url = article.find(\"a\")[\"href\"]\n",
    "\n",
    "    # Store the extracted information as a dictionary\n",
    "    article_dict = {\n",
    "        \"Paper Title\": title,\n",
    "        \"Authors\": authors,\n",
    "        \"Published Date\": published,\n",
    "        \"Paper URL\": url\n",
    "    }\n",
    "\n",
    "    # Add the dictionary to the list of article data\n",
    "    article_data.append(article_dict)\n",
    "\n",
    "# Create a Pandas dataframe to store the article data\n",
    "df = pd.DataFrame(article_data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a09e8d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m urls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Iterate over each article in the container\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43marticles_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     23\u001b[0m   \u001b[38;5;66;03m# Scrape the title\u001b[39;00m\n\u001b[0;32m     24\u001b[0m   title \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m   titles\u001b[38;5;241m.\u001b[39mappend(title)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container that holds the article details\n",
    "articles_container = soup.find(\"div\", class_=\"pod-listing\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "# Iterate over each article in the container\n",
    "for article in articles_container.find_all(\"li\"):\n",
    "  # Scrape the title\n",
    "  title = article.find(\"h3\").text.strip()\n",
    "  titles.append(title)\n",
    "  \n",
    "  # Scrape the authors\n",
    "  author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "  authors.append(author)\n",
    "  \n",
    "  # Scrape the published date\n",
    "  date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "  dates.append(date)\n",
    "  \n",
    "  # Scrape the paper URL\n",
    "  url = article.find(\"a\")[\"href\"]\n",
    "  urls.append(url)\n",
    "\n",
    "# Create a dataframe with the scraped data\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame.\n",
    "#i) Restaurant name\n",
    "#ii) Cuisine\n",
    "#iii) Location\n",
    "#iv) Ratings\n",
    "#v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134e47eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.dineout.co.in/delhi\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the elements containing the details you want to scrape\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "# Create empty lists to store the scraped data\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "# Extract the data from the elements and append them to the respective lists\n",
    "for name in restaurant_names:\n",
    "  restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "  cuisine_list.append(cuisine.text.strip())\n",
    "\n",
    "for location in locations:\n",
    "  location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "  rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "  image_url_list.append(image['src'])\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687d6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
