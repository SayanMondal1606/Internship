{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c321e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b479eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c88e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf442d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    url = f\"https://www.amazon.in/s?k={product}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception if request is unsuccessful\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    product_cards = soup.find_all(\"div\", class_=\"sg-col-inner\")\n",
    "    \n",
    "    for card in product_cards:\n",
    "        try:\n",
    "            title = card.find(\"span\", class_=\"a-size-medium\").get_text()\n",
    "            price = card.find(\"span\", class_=\"a-price-whole\").get_text()\n",
    "            link = \"https://www.amazon.in\" + card.find(\"a\", class_=\"a-link-normal\")['href']\n",
    "            \n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Price: {price}\")\n",
    "            print(f\"Link: {link}\\n\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search_term = input(\"Enter a product to search on Amazon.in: \")\n",
    "    search_amazon(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72360e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce215837",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_xpath('Write the specific X_path')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(inputU)       # Inputing keyword to search \n",
    "search_button = driver.find_element_by_xpath('Write the specific X_path')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebf505",
   "metadata": {},
   "outputs": [],
   "source": [
    "productName=[]\n",
    "#scraping the Product_Name \n",
    "PName=driver.find_elements_by_xpath(\"Write the specific X_path\")\n",
    "for i in PName:\n",
    "    if i.text is None :\n",
    "        productName.append(\"--\") \n",
    "    else:\n",
    "        productName.append(i.text)\n",
    "print(len(productName),productName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = 0\n",
    "end_page = 3\n",
    "urls = []\n",
    "for page in range(start_page,end_page+1):\n",
    "    try:\n",
    "        page_urls = driver.find_elements_by_xpath('Write the specific X_path')\n",
    "        \n",
    "        # appending all the urls on current page to urls list\n",
    "        for url in page_urls:\n",
    "            url = url.get_attribute('href')     # Scraping the url from webelement\n",
    "            if url[0:4]=='http':                # Checking if the scraped data is a valid url or not\n",
    "                urls.append(url)                # Appending the url to urls list\n",
    "        print(\"Product urls of page {} has been scraped.\".format(page+1))\n",
    "        \n",
    "        # Moving to next page\n",
    "        nxt_button = driver.find_element_by_xpath('Write the specific X_path')      # Locating the next_button which is active\n",
    "        if nxt_button.text == 'Next→':                                            # Checking if the button located is next button\n",
    "            nxt_button.click()                                                    # Clicking the next button\n",
    "            time.sleep(5)                                                         # time delay of 5 seconds\n",
    "        # If the current active button is not next button, the we will check if the next button is inactive or not    \n",
    "        elif driver.find_element_by_xpath('Write the specific X_path').text == 'Next→':    \n",
    "            print(\"No new pages exist. Breaking the loop\")  # Printing message and breakinf loop if we have reached the last page\n",
    "            break\n",
    "            \n",
    "    except StaleElementReferenceException as e:             # Handling StaleElement Exception   \n",
    "        print(\"Stale Exception\")\n",
    "        next_page = nxt_button.get_attribute('href')        # Extracting the url of next page\n",
    "        driver.get(next_page)                               # ReLoading the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_dict = {}\n",
    "prod_dict['Brand']=[]\n",
    "prod_dict['Name']=[]\n",
    "prod_dict['Rating']=[]\n",
    "prod_dict['No. of ratings']=[]\n",
    "prod_dict['Price']=[]\n",
    "prod_dict['Return/Exchange']=[]\n",
    "prod_dict['Expected Delivery']=[] \n",
    "prod_dict['Availability']=[]\n",
    "prod_dict['Other Details']=[]\n",
    "prod_dict['URL']=[]\n",
    "for url in urls[:4]:\n",
    "    driver.get(url)                                                        # Loading the webpage by url\n",
    "    print(\"Scraping URL = \", url)\n",
    "    #time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('Write the specific X_path')      # Extracting Brand from xpath\n",
    "        prod_dict['Brand'].append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Brand'].append('-')\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('Write the specific X_path')      # Extracting Name from xpath\n",
    "        prod_dict['Name'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Name'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath('Write the specific X_path')  # Extracting Ratings from xpath\n",
    "        prod_dict['Rating'].append(rating.get_attribute(\"title\"))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Rating'].append('-')\n",
    "    \n",
    "    try:\n",
    "        n_rating = driver.find_element_by_xpath('Write the specific X_path')     # Extracting no. of Ratings from xpath\n",
    "        prod_dict['No. of ratings'].append(n_rating.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['No. of ratings'].append('-')\n",
    "    \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('Write the specific X_path')            # Extracting Price from xpath\n",
    "        prod_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Price'].append('-')\n",
    "    try:                                                                                     # Extracting Return/Exchange policy from xpath\n",
    "        ret = driver.find_element_by_xpath('Write the specific X_path')\n",
    "        prod_dict['Return/Exchange'].append(ret.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Return/Exchange'].append('-')\n",
    "    try:\n",
    "        delivry = driver.find_element_by_xpath('Write the specific X_path')         # Extracting Expected Delivery from xpath\n",
    "        prod_dict['Expected Delivery'].append(delivry.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Expected Delivery'].append('-')\n",
    "    \n",
    "    try:\n",
    "        avl = driver.find_element_by_xpath('Write the specific X_path')                # Extracting Availability from xpath\n",
    "        prod_dict['Availability'].append(avl.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Availability'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        dtls = driver.find_element_by_xpath('Write the specific X_path')\n",
    "        prod_dict['Other Details'].append('  ||  '.join(dtls.text.split('\\n')))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Other Details'].append('-')\n",
    "    \n",
    "    prod_dict['URL'].append(url)                                                            # Saving url\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.DataFrame.from_dict(prod_dict)\n",
    "prod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21aa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def scrape_images(keyword):\n",
    "    # Initialize Chrome WebDriver (make sure webdriver executable is in PATH)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://images.google.com/')\n",
    "\n",
    "    # Find the search bar element and enter the keyword\n",
    "    search_bar = driver.find_element_by_name('q')\n",
    "    search_bar.send_keys(keyword)\n",
    "\n",
    "    # Find the search button element and click it\n",
    "    search_button = driver.find_element_by_css_selector('input[type=\"submit\"][value=\"Search\"]')\n",
    "    search_button.click()\n",
    "\n",
    "    # Scroll down to load more images\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Find all image elements, wait till they are loaded\n",
    "    images = driver.find_elements_by_css_selector('img.rg_i')\n",
    "    while len(images) < 10:\n",
    "        images = driver.find_elements_by_css_selector('img.rg_i')\n",
    "\n",
    "    # Extract image URLs and store them in a list\n",
    "    image_urls = []\n",
    "    for image in images[:10]:\n",
    "        image_urls.append(image.get_attribute('src'))\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    image_urls = scrape_images(keyword)\n",
    "    print(f'{keyword} images:')\n",
    "    for url in image_urls:\n",
    "        print(url)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_product_details(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Get brand and smartphone name\n",
    "    title = soup.find('span', class_='_35KyD6').text.split(' ')\n",
    "    data['Brand Name'] = title[0]\n",
    "    data['Smartphone Name'] = ' '.join(title[1:])\n",
    "    \n",
    "    # Get other details\n",
    "    details = soup.find_all('ul', class_='_1xgFaf')\n",
    "    for detail in details:\n",
    "        items = detail.find_all('li')\n",
    "        for item in items:\n",
    "            key = item.find('div', class_='_3wU53n').text\n",
    "            value = item.find('div', class_='_2kHMtA').text\n",
    "            data[key] = value\n",
    "    \n",
    "    # Get product URL\n",
    "    data['Product URL'] = url\n",
    "    \n",
    "    return data\n",
    "\n",
    "def search_smartphone(search_query):\n",
    "    base_url = 'https://www.flipkart.com/search'\n",
    "    query = search_query.replace(' ', '%20')\n",
    "    url = f'{base_url}?q={query}&sid=tyy%2C4io&p%5B%5D=facets.serviceability%5B%5D%3Dtrue&sort=price_asc'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='_31qSD5')\n",
    "    data_list = []\n",
    "    \n",
    "    for product in products:\n",
    "        product_url = 'https://www.flipkart.com' + product['href']\n",
    "        data = get_product_details(product_url)\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "# Search for smartphone\n",
    "search_query = input('Enter smartphone name: ')\n",
    "data_list = search_smartphone(search_query)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Replace missing values with '-'\n",
    "df.fillna('-', inplace=True)\n",
    "\n",
    "# Save dataframe to CSV\n",
    "df.to_csv('smartphones.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aaf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "# opening google maps\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "time.sleep(3)\n",
    "\n",
    "city = input('Enter City Name : ')                                         # Enter city to be searched\n",
    "search = driver.find_element_by_xpath(\"Write the specific X_path\")   # locating search bar                     # locating search bar\n",
    "search.clear()                                                             # clearing search bar\n",
    "time.sleep(2)\n",
    "search.send_keys(city)                                                     # entering values in search bar\n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")               # locating search button\n",
    "button.click()                                                             # clicking search button\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dace287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7602bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4dd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.digit.in/top-products/best-budget-gaming-laptop-3591.html\")\n",
    "Brands=[]\n",
    "Products_Description=[]\n",
    "Specification=[]\n",
    "Price=[]\n",
    "br = driver.find_elements_by_xpath(By.XPATH,\"Write the specific X_path\")\n",
    "\n",
    "len(br)\n",
    "for i in br:\n",
    "   \n",
    "    Brands.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in br:\n",
    "   \n",
    "    Brands.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d51e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=driver.find_elements_by_xpath(\"Write the specific X_path\")\n",
    "len(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82938ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sp:\n",
    "   \n",
    "    Specification.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52050eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "des=driver.find_elements_by_xpath(\"Write the specific X_path\")\n",
    "len(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8581dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in des:\n",
    "   \n",
    "    Products_Description.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Products_Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71952e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pri=driver.find_elements_by_xpath(\"Write the specific X_path\")\n",
    "len(pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dec632",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pri:\n",
    "   \n",
    "    Price.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3480805",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_lap=pd.DataFrame([])\n",
    "digit_lap['Brands']=Brands[0:10]\n",
    "digit_lap['Price']=Price[0:10]\n",
    "digit_lap['Specification']=Specification[0:10]\n",
    "digit_lap['Description']=Products_Description[0:10]\n",
    "digit_lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd98fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a633f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Forbes website\n",
    "url = \"https://www.forbes.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the billionaire details\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "# Iterate over each row and extract the required details\n",
    "for row in rows:\n",
    "  # Find the columns in each row\n",
    "  columns = row.find_all(\"td\")\n",
    "  \n",
    "  # Extract the required details from the columns\n",
    "  rank = columns[0].text.strip()\n",
    "  name = columns[1].text.strip()\n",
    "  net_worth = columns[2].text.strip()\n",
    "  age = columns[3].text.strip()\n",
    "  citizenship = columns[4].text.strip()\n",
    "  source = columns[5].text.strip()\n",
    "  industry = columns[6].text.strip()\n",
    "  \n",
    "  # Print the extracted details\n",
    "  print(\"Rank:\", rank)\n",
    "  print(\"Name:\", name)\n",
    "  print(\"Net worth:\", net_worth)\n",
    "  print(\"Age:\", age)\n",
    "  print(\"Citizenship:\", citizenship)\n",
    "  print(\"Source:\", source)\n",
    "  print(\"Industry:\", industry)\n",
    "  print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Enter your YouTube Data API key\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "\n",
    "def get_video_comments(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "    # Retrieve the comments from the video\n",
    "    response = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        order='relevance',\n",
    "        maxResults=100\n",
    "    ).execute()\n",
    "\n",
    "    comments = []\n",
    "\n",
    "    while response:\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'comment': comment['textDisplay'],\n",
    "                'upvotes': comment['likeCount'],\n",
    "                'time': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        # Check if there are more pages of comments\n",
    "        if 'nextPageToken' in response:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                textFormat='plainText',\n",
    "                order='relevance',\n",
    "                maxResults=100,\n",
    "                pageToken=response['nextPageToken']\n",
    "            ).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Extract YouTube video comments')\n",
    "    parser.add_argument('video_id', help='YouTube video id')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    video_comments = get_video_comments(args.video_id)\n",
    "\n",
    "    # Display the extracted comments\n",
    "    for comment in video_comments:\n",
    "        print('Comment:', comment['comment'])\n",
    "        print('Upvotes:', comment['upvotes'])\n",
    "        print('Time:', comment['time'])\n",
    "        print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.hostelworld.com/hostels/London\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "hostel_listings = soup.find_all(\"div\", class_=\"fabresult\")\n",
    "\n",
    "for hostel in hostel_listings:\n",
    "\n",
    "  name = hostel.find(\"h2\", class_=\"title\").text.strip()\n",
    "\n",
    "  distance = hostel.find(\"span\", class_=\"distance\").text.strip()\n",
    "\n",
    "  ratings = hostel.find(\"div\", class_=\"score orange\").text.strip()\n",
    "\n",
    "  total_reviews = hostel.find(\"div\", class_=\"reviews\").text.strip()\n",
    "\n",
    "  overall_reviews = hostel.find(\"div\", class_=\"rating\").text.strip()\n",
    "\n",
    "  privates_price = hostel.find(\"div\", class_=\"price\").text.strip()\n",
    "\n",
    "  dorms_price = hostel.find(\"div\", class_=\"price dorms\").text.strip()\n",
    "\n",
    "  facilities = [facility.text.strip() for facility in hostel.find_all(\"span\", class_=\"facilities\")]\n",
    "\n",
    "  description = hostel.find(\"div\", class_=\"desc\").text.strip()\n",
    "\n",
    "  print(\"Name:\", name)\n",
    "  print(\"Distance from city centre:\", distance)\n",
    "  print(\"Ratings:\", ratings)\n",
    "  print(\"Total reviews:\", total_reviews)\n",
    "  print(\"Overall reviews:\", overall_reviews)\n",
    "  print(\"Privates from price:\", privates_price)\n",
    "  print(\"Dorms from price:\", dorms_price)\n",
    "  print(\"Facilities:\", facilities)\n",
    "  print(\"Description:\", description)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
